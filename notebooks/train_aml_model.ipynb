{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Deploy AML Model\n",
    "\n",
    "This notebook trains a LogisticRegression model to predict high-value donors (total_donations > $1000) using `gold/donor_activity` and deploys it to an Azure Machine Learning endpoint.\n",
    "\n",
    "## Inputs\n",
    "- `gold/donor_activity` (from `silver_to_gold`)\n",
    "\n",
    "## Outputs\n",
    "- Model saved to `models/high_value_donor_model`\n",
    "- AML endpoint for real-time predictions\n",
    "\n",
    "## Dependencies\n",
    "- `pyspark.ml`, `azure-ai-ml`, `azure-identity`\n",
    "\n",
    "## Environment\n",
    "- Uses `.env` for Blob Storage and AML credentials\n",
    "- Runs monthly in Databricks Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import ManagedOnlineEndpoint, ManagedOnlineDeployment, Model\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_CONN_STR = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.getenv(\"CONTAINER_NAME\")\n",
    "AZURE_STORAGE_ACCOUNT_NAME = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "AZURE_RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "AZURE_ML_WORKSPACE = os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    "\n",
    "if not all([AZURE_CONN_STR, CONTAINER_NAME, AZURE_STORAGE_ACCOUNT_NAME, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_ML_WORKSPACE]):\n",
    "    raise ValueError(\"Missing environment variables. Check .env file.\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"TrainAMLModel\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.account.auth.type\", \"SAS\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.sas.token.provider.type\", \"org.apache.hadoop.fs.azure.SimpleSasTokenProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.sas.fixed.token\", AZURE_CONN_STR) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define blob storage paths\n",
    "blob_base_path = f\"wasbs://{CONTAINER_NAME}@{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "gold_path = f\"{blob_base_path}/gold\"\n",
    "model_path = f\"{blob_base_path}/models/high_value_donor_model\"\n",
    "\n",
    "# Read donor_activity table\n",
    "donor_features = spark.read.parquet(f\"{gold_path}/donor_activity\")\n",
    "\n",
    "# Prepare features and label\n",
    "features = donor_features.withColumn(\"label\", when(col(\"total_donations\") > 1000, 1).otherwise(0))\n",
    "assembler = VectorAssembler(inputCols=[\"total_donations\", \"donation_count\"], outputCol=\"features\")\n",
    "data = assembler.transform(features)\n",
    "\n",
    "# Train LogisticRegression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "model = lr.fit(data)\n",
    "\n",
    "# Save model to Blob Storage\n",
    "model.write(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Initialize AML client\n",
    "credential = DefaultAzureCredential()\n",
    "ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_ML_WORKSPACE)\n",
    "\n",
    "# Create or update AML endpoint\n",
    "endpoint_name = \"high-value-donor-endpoint\"\n",
    "endpoint = ManagedOnlineEndpoint(name=endpoint_name)\n",
    "try:\n",
    "    ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "except Exception as e:\n",
    "    print(f\"Endpoint creation error: {str(e)}\")\n",
    "\n",
    "# Deploy model to AML endpoint\n",
    "deployment_name = \"high-value-donor-deployment\"\n",
    "model_entity = Model(path=model_path)\n",
    "deployment = ManagedOnlineDeployment(\n",
    "    name=deployment_name,\n",
    "    endpoint_name=endpoint_name,\n",
    "    model=model_entity,\n",
    "    instance_type=\"Standard_DS2_v2\",\n",
    "    instance_count=1\n",
    ")\n",
    "try:\n",
    "    ml_client.online_deployments.begin_create_or_update(deployment).result()\n",
    "    # Set deployment to handle 100% of traffic\n",
    "    ml_client.online_endpoints.update_traffic(endpoint_name, {deployment_name: 100})\n",
    "    # Get endpoint details\n",
    "    endpoint_details = ml_client.online_endpoints.get(endpoint_name)\n",
    "    scoring_uri = endpoint_details.scoring_uri\n",
    "    endpoint_key = ml_client.online_endpoints.get_keys(endpoint_name).primary_key\n",
    "    print(f\"Model deployed to AML endpoint: {scoring_uri}\")\n",
    "    print(f\"Update .env with:\\nAZURE_ML_ENDPOINT_URL={scoring_uri}\\nAZURE_ML_ENDPOINT_KEY={endpoint_key}\")\n",
    "except Exception as e:\n",
    "    print(f\"Deployment error: {str(e)}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"AML model training and deployment completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Silver to Gold Processing\n",
    "\n",
    "This notebook aggregates data from `silver/` to create analytical tables in `gold/`, uses Azure Machine Learning for high-value donor predictions, and writes to Azure Synapse Analytics SQL pool for Power BI reporting.\n",
    "\n",
    "## Inputs\n",
    "- `silver/<table>/` (e.g., `donations`, `crm_data`)\n",
    "- AML endpoint for predictions\n",
    "\n",
    "## Outputs\n",
    "- `gold/<table>/` (e.g., `donation_trends`, `predicted_donations`)\n",
    "- Synapse SQL pool tables\n",
    "\n",
    "## Dependencies\n",
    "- `pyspark`, `azure-ai-ml`, `azure-identity`, `requests`\n",
    "\n",
    "## Environment\n",
    "- Uses `.env` for Blob Storage, AML, and Synapse credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, datediff, hour, round, when, floor, date_trunc, current_date\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "AZURE_CONN_STR = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "CONTAINER_NAME = os.getenv(\"CONTAINER_NAME\")\n",
    "AZURE_STORAGE_ACCOUNT_NAME = os.getenv(\"AZURE_STORAGE_ACCOUNT_NAME\")\n",
    "AZURE_SUBSCRIPTION_ID = os.getenv(\"AZURE_SUBSCRIPTION_ID\")\n",
    "AZURE_RESOURCE_GROUP = os.getenv(\"AZURE_RESOURCE_GROUP\")\n",
    "AZURE_ML_WORKSPACE = os.getenv(\"AZURE_ML_WORKSPACE\")\n",
    "AZURE_ML_ENDPOINT_URL = os.getenv(\"AZURE_ML_ENDPOINT_URL\")\n",
    "AZURE_ML_ENDPOINT_KEY = os.getenv(\"AZURE_ML_ENDPOINT_KEY\")\n",
    "SYNAPSE_WORKSPACE_NAME = os.getenv(\"SYNAPSE_WORKSPACE_NAME\")\n",
    "SYNAPSE_DATABASE = os.getenv(\"SYNAPSE_DATABASE\")\n",
    "SYNAPSE_USER = os.getenv(\"SYNAPSE_USER\")\n",
    "SYNAPSE_PASSWORD = os.getenv(\"SYNAPSE_PASSWORD\")\n",
    "\n",
    "if not all([AZURE_CONN_STR, CONTAINER_NAME, AZURE_STORAGE_ACCOUNT_NAME, AZURE_ML_ENDPOINT_URL, AZURE_ML_ENDPOINT_KEY, SYNAPSE_WORKSPACE_NAME]):\n",
    "    raise ValueError(\"Missing environment variables. Check .env file.\")\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SilverToGold\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.account.auth.type\", \"SAS\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.sas.token.provider.type\", \"org.apache.hadoop.fs.azure.SimpleSasTokenProvider\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.sas.fixed.token\", AZURE_CONN_STR) \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define blob storage paths\n",
    "blob_base_path = f\"wasbs://{CONTAINER_NAME}@{AZURE_STORAGE_ACCOUNT_NAME}.blob.core.windows.net\"\n",
    "silver_path = f\"{blob_base_path}/silver\"\n",
    "gold_path = f\"{blob_base_path}/gold\"\n",
    "\n",
    "# Synapse connection properties\n",
    "synapse_jdbc = f\"jdbc:sqlserver://{SYNAPSE_WORKSPACE_NAME}.sql.azuresynapse.net:1433;database={SYNAPSE_DATABASE};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n",
    "synapse_properties = {\n",
    "    \"user\": SYNAPSE_USER,\n",
    "    \"password\": SYNAPSE_PASSWORD,\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}\n",
    "\n",
    "# Read silver tables\n",
    "donations = spark.read.parquet(f\"{silver_path}/donations\")\n",
    "projects = spark.read.parquet(f\"{silver_path}/projects\")\n",
    "volunteer_shifts = spark.read.parquet(f\"{silver_path}/volunteer_shifts\")\n",
    "campaigns = spark.read.parquet(f\"{silver_path}/campaigns\")\n",
    "volunteers = spark.read.parquet(f\"{silver_path}/volunteers\")\n",
    "beneficiaries = spark.read.parquet(f\"{silver_path}/beneficiaries\")\n",
    "transactions = spark.read.parquet(f\"{silver_path}/transactions\")\n",
    "crm_data = spark.read.parquet(f\"{silver_path}/crm_data\")\n",
    "\n",
    "# Aggregations\n",
    "donations_per_project = donations.join(projects, \"project_id\") \\\n",
    "    .groupBy(\"project_id\", \"project_name\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\")\n",
    "    )\n",
    "\n",
    "volunteer_hours = volunteer_shifts.join(projects, \"project_id\") \\\n",
    "    .groupBy(\"project_id\", \"project_name\") \\\n",
    "    .agg(sum(\"shift_duration_hours\").alias(\"total_hours\"))\n",
    "\n",
    "donations_by_region = donations.join(projects, \"project_id\") \\\n",
    "    .groupBy(\"region\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\")\n",
    "    )\n",
    "\n",
    "campaign_performance = donations.join(campaigns, \"campaign_id\") \\\n",
    "    .groupBy(\"campaign_id\", \"title\", \"target_amount\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\")\n",
    "    ) \\\n",
    "    .withColumn(\"percent_target_achieved\", round((col(\"total_donations\") / col(\"target_amount\")) * 100, 2))\n",
    "\n",
    "donor_activity = donations.groupBy(\"donor_id\", \"donor_name\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_donations\").desc())\n",
    "\n",
    "volunteer_engagement = volunteer_shifts.join(volunteers, \"volunteer_id\") \\\n",
    "    .withColumn(\"age_group\", \n",
    "        when(col(\"age\").between(16, 25), \"16-25\")\n",
    "        .when(col(\"age\").between(26, 35), \"26-35\")\n",
    "        .when(col(\"age\").between(36, 45), \"36-45\")\n",
    "        .when(col(\"age\").between(46, 55), \"46-55\")\n",
    "        .when(col(\"age\").between(56, 65), \"56-65\")\n",
    "        .when(col(\"age\").between(66, 80), \"66-80\")\n",
    "        .otherwise(\"Unknown\")\n",
    "    ) \\\n",
    "    .groupBy(\"age_group\") \\\n",
    "    .agg(\n",
    "        sum(\"shift_duration_hours\").alias(\"total_hours\"),\n",
    "        count(\"*\").alias(\"shift_count\")\n",
    "    )\n",
    "\n",
    "beneficiary_demographics = beneficiaries.groupBy(\"aid_type\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"beneficiary_count\"),\n",
    "        round(avg(\"age\"), 2).alias(\"average_age\")\n",
    "    )\n",
    "\n",
    "transaction_success_rate = transactions.groupBy(\"payment_provider\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_transactions\"),\n",
    "        sum(when(col(\"status\") == \"Success\", 1).otherwise(0)).alias(\"successful_transactions\")\n",
    "    ) \\\n",
    "    .withColumn(\"success_rate\", round((col(\"successful_transactions\") / col(\"total_transactions\")) * 100, 2))\n",
    "\n",
    "donation_trends = donations.groupBy(date_trunc(\"month\", \"donation_date\").alias(\"month\")) \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\")\n",
    "    ) \\\n",
    "    .orderBy(\"month\")\n",
    "\n",
    "active_campaigns = campaigns.join(projects, \"project_id\", \"left\") \\\n",
    "    .filter(col(\"end_date\") >= current_date()) \\\n",
    "    .groupBy(\"region\") \\\n",
    "    .agg(count(\"*\").alias(\"active_campaign_count\"))\n",
    "\n",
    "# CRM-based donor engagement score\n",
    "donor_engagement = donations.join(crm_data, \"donor_id\") \\\n",
    "    .groupBy(\"donor_id\", \"donor_name\", \"email\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"contact_count\"),\n",
    "        max(\"last_contact_date\").alias(\"last_contact\")\n",
    "    ) \\\n",
    "    .withColumn(\"engagement_score\", \n",
    "        when(datediff(current_date(), col(\"last_contact\")) < 30, 3)\n",
    "        .when(datediff(current_date(), col(\"last_contact\")) < 90, 2)\n",
    "        .otherwise(1) * col(\"contact_count\")\n",
    "    )\n",
    "\n",
    "# AML-based high-value donor predictions\n",
    "donor_features = donations.groupBy(\"donor_id\", \"donor_name\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_donations\"),\n",
    "        count(\"*\").alias(\"donation_count\"),\n",
    "        avg(\"amount\").alias(\"avg_donation\")\n",
    "    )\n",
    "try:\n",
    "    assembler = VectorAssembler(inputCols=[\"total_donations\", \"donation_count\", \"avg_donation\"], outputCol=\"features\")\n",
    "    feature_data = assembler.transform(donor_features)\n",
    "    feature_json = feature_data.select(\"donor_id\", \"donor_name\", \"features\").toJSON().collect()\n",
    "    predictions = []\n",
    "    for row in feature_json:\n",
    "        row_dict = json.loads(row)\n",
    "        features = row_dict[\"features\"]\n",
    "        headers = {\"Authorization\": f\"Bearer {AZURE_ML_ENDPOINT_KEY}\", \"Content-Type\": \"application/json\"}\n",
    "        payload = {\"input_data\": features}\n",
    "        response = requests.post(AZURE_ML_ENDPOINT_URL, json=payload, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            pred = response.json()\n",
    "            predictions.append((row_dict[\"donor_id\"], row_dict[\"donor_name\"], float(pred[\"prediction\"])))\n",
    "        else:\n",
    "            print(f\"AML endpoint error: {response.text}\")\n",
    "    predicted_donations = spark.createDataFrame(\n",
    "        predictions,\n",
    "        [\"donor_id\", \"donor_name\", \"high_value_likelihood\"]\n",
    "    ).join(donor_features, [\"donor_id\", \"donor_name\"])\n",
    "except Exception as e:\n",
    "    print(f\"AML prediction error: {str(e)}. Skipping predicted_donations.\")\n",
    "    predicted_donations = None\n",
    "\n",
    "# Write to gold layer (Blob Storage)\n",
    "donations_per_project.write.mode(\"overwrite\").parquet(f\"{gold_path}/donations_per_project\")\n",
    "volunteer_hours.write.mode(\"overwrite\").parquet(f\"{gold_path}/volunteer_hours_per_project\")\n",
    "donations_by_region.write.mode(\"overwrite\").parquet(f\"{gold_path}/donations_by_region\")\n",
    "campaign_performance.write.mode(\"overwrite\").parquet(f\"{gold_path}/campaign_performance\")\n",
    "donor_activity.write.mode(\"overwrite\").parquet(f\"{gold_path}/donor_activity\")\n",
    "volunteer_engagement.write.mode(\"overwrite\").parquet(f\"{gold_path}/volunteer_engagement\")\n",
    "beneficiary_demographics.write.mode(\"overwrite\").parquet(f\"{gold_path}/beneficiary_demographics\")\n",
    "transaction_success_rate.write.mode(\"overwrite\").parquet(f\"{gold_path}/transaction_success_rate\")\n",
    "donation_trends.write.mode(\"overwrite\").parquet(f\"{gold_path}/donation_trends\")\n",
    "active_campaigns.write.mode(\"overwrite\").parquet(f\"{gold_path}/active_campaigns\")\n",
    "donor_engagement.write.mode(\"overwrite\").parquet(f\"{gold_path}/donor_engagement\")\n",
    "if predicted_donations:\n",
    "    predicted_donations.write.mode(\"overwrite\").parquet(f\"{gold_path}/predicted_donations\")\n",
    "\n",
    "# Write to Synapse dedicated SQL pool\n",
    "try:\n",
    "    donations_per_project.write.jdbc(synapse_jdbc, \"donations_per_project\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    donations_by_region.write.jdbc(synapse_jdbc, \"donations_by_region\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    campaign_performance.write.jdbc(synapse_jdbc, \"campaign_performance\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    donor_activity.write.jdbc(synapse_jdbc, \"donor_activity\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    donation_trends.write.jdbc(synapse_jdbc, \"donation_trends\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    donor_engagement.write.jdbc(synapse_jdbc, \"donor_engagement\", mode=\"overwrite\", properties=synapse_properties)\n",
    "    if predicted_donations:\n",
    "        predicted_donations.write.jdbc(synapse_jdbc, \"predicted_donations\", mode=\"overwrite\", properties=synapse_properties)\n",
    "except Exception as e:\n",
    "    print(f\"Synapse write error: {str(e)}.\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"Silver to Gold processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}